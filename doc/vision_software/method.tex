% Do NOT change this "Section" title
% and do NOT add more "Section" level titles.
\section{Method}\label{sec:method}
There are two parts to the following section. Firstly the preparation steps undertaken to integrate all software components with each other is described. Secondly some of the main software modules implemented are described.
\subsection{Preparation}
Before the modules need for the visual perception system could be coded, the software modules needed to be interfaced to each other. The following subsections describes the work undertaken to prepare all the software modules.
\subsubsection{OpenCV with Ada}
The first task undertaken in this project was to try to get OpenCV working with Ada. Bindings were found online \cite{web:oldAdaBindings} however these binding were found to be rigidly implemented to an older version of OpenCV and an operating system no longer supported. An alternative solution was required. After much research a method of binding C++ to Ada was found \cite{web:newAdaBindings} and this method was adapted to bind OpenCV to Ada.

This generated bindings that could be used by including the wrapper file in the ada project, however linking several projects that used the bindings together, and compiling and running the project in an IDE proved troublesome. A Makefile was constructed to link everything together.

\subsubsection{PCL with Ada}
Much like OpenCV, PCL needed to be converted from C++ to Ada. The same approach as OpenCV was undertaken. The PCL modules required were first developed in C++ and then Ada bindings were generated using the binding commands found \cite{web:newAdaBindings}. The PCL functions could then be called from Ada. The Makefile was then adjusted to incorporate PCL. This simply required adding include directories to the necessary PCL libraries in the Makefile 

\subsubsection{Developing for the GIMME-2}
As the GIMME-2 was a state of the art hardware platform, development support was limited. One positive was that its ARM processor architecture is also used in the BeagleBone Black and Pandaboard. As the Ubuntu operating system was not supported by GIMME-2, it was decided to compile the binaries needed on a Pandaboard statically and then move the binaries to the GIMME-2 to be run.

\subsection{Software}
The visual and perception system was broken down into separate modules for each vision component needed. This was done to modularize the system, making it easy to add and remove modules if and as they were required.  
\subsubsection{Stereo Vision}
The hardware is potentially capable of stereo vision, taking pictures with its dual camera lenses at the same time, which enables depth perception and motion calculations.

\begin{description}
  \item [Calibrating the Cameras] \hfill \\As the GIMME-2 was having problems taking two pictures simultaneously, a custom stereo rig was constructed to run and test the calibration software. Calibration and Rectification code was constructed using a chessboard image to calibrate the cameras. This code can be tested on the GIMME-2 when it was the ability to take 10 consecutive stereo images needed to run the calibration software.
  \item [Disparity Maps] \hfill \\Since the GIMME-2 flaunts a stereo system, disparity maps can be used for motion and distance estimation. OpenCV functionalities were used for this purpose.

\end{description}

\subsubsection{Image Enhancement}
Cleaning up the image was attempted by four methods. Blurring the image, sharpening the image to make features more clear and easier to detect, manipulating image properties, and noise reduction. 
\begin{description}
\item[Median Filter]\hfill \\The idea behind the Median filter is to search for highly improbable pixel values, and replace them with the median value of the surrounding pixels \cite{davies2012computer}.

The median filter makes a 3x3 sliding window that reduces noise by filtering every pixel of the image. It works by replacing the value at the centre of the window with the median value of the pixels in the window.

If the window N is centred at (i, j), the median filter can be represented by the following equation as shown in Forsyth2011 \cite{forsyth2011computer}:
\begin{equation}
y_{ij}=med(\lbrace x_{uv}|x_{uv}\in N_{ij} \rbrace)
\end{equation}

\item[Quaternion Switching Filter]\hfill \\
The quaternion switching filter \cite{Geng2012150} is a filter based on quaternions.
A quaternion is a vector representation that has 1 real part and 3 imaginary parts as Hamilton discovered \cite{article:Hamilton1844} of the form:
\begin{equation}
Q=w+ix+jy+kz
\end{equation}

Quaternions are useful in image processing as an RGB pixel can be represented as a purely imaginary quaternion by letting the real part of the vector equal zero. A quaternion with only imaginary parts is known as a right quaternion \cite{article:Hamilton1866}.
The quaternions allow representing a 3 value RGB pixel as a single function. Then quaternion unit transform theory can be used to find the pixel chromaticity difference and pixel intensity difference for the pixels in the sliding window.

The intensity difference and chromaticity difference can be represented as shown in Geng2012 \cite{Geng2012150}:
\\
\\ Where
\\ \textit{r, g, b: red, green and blue color components with a range between 0 and 255}
\\ \textit{i, j, k: imaginary co-efficients}
\begin{equation}
\bar{T}qT = \frac{1}{3}(r+g+b)(i+j+k)-\frac{1}{\sqrt{3}}[(b-g)i+(r-b)j+(g-r)k]
\end{equation}
\begin{equation}
Tq\bar{T} = \frac{1}{3}(r+g+b)(i+j+k)+\frac{1}{\sqrt{3}}[(b-g)i+(r-b)j+(g-r)k]
\end{equation}
\\
\\
The intensity difference:
\begin{equation}
d_1(q_1,q_2)=(\frac{1}{2}|[(Tq_1\bar{T}+\bar{T}q_1T)-(Tq_2\bar{T}+\bar{T}q_2T)])
\end{equation}
resolves to
\begin{equation}
\frac{1}{3}(i+j+k)(r+g+b)
\end{equation}
and the chromaticity difference
\begin{equation}
d_2(q_1,q_2)=(\frac{1}{2}|[(Tq_1\bar{T}-\bar{T}q_1T)-(Tq_2\bar{T}-\bar{T}q_2T)])
\end{equation}
resolves to
\begin{equation}
\frac{1}{\sqrt{3}}[(b-g)i+(r-b)j+(g-r)k]
\end{equation}

If the combined greatest difference, $d_1 + d_2$, is greater than a user defined threshold then the pixel is subjected to a Vector Median Filter. If the difference is less than the threshold then no filtering is performed on the pixel.
\end{description}
 
\subsubsection{Manipulating Image Properties} 
Image properties were manipulated in order to enhance certain features of images. This helped improve the performance of the object detection modules.
\begin{description}
\item[Image Contrast]\hfill \\
This is a basic image manipulation functionality targeted to enhance the image quality if required.
It makes use of the OpenCV function \emph{convertTo} using alpha and beta as gain and bias parameters respetively.

\item[Invert Image]\hfill \\
In this context, \emph{Invert} refers to the reversal in the nature of the pixel intensities of the image.
For instance, an originally pure black input pixel yields a pure white pixel as output.
It makes use of the simple form of  
\begin{equation} V(new) = 255 - V(old)  
\end{equation} 
where V is the value of the pixel in BGR, to invert an image. 
This method is particularly helpful when working with masks.

In the final implementation, the OpenCV function \emph{addWeighted} was used to achieve the desired functionality, with constants 1.00 and -1.00 for alpha and beta channels respectively.
\begin{align*}
addWeighted(mask, 1.0, src, -1.0, 0, dst)
\end{align*}
where the mask is a pure white image.

\item[Split Channels]\hfill \\
This splits an input image into it's basic color components, so the system could have color-channel filtering without use of any extra hardware.

\item[Gaussian Blur]\hfill \\
The Gaussian Blur function \cite{web:gaussianBlur}  needed to be implemented for the edge detection modules. A blur basically smooths out the image and makes jagged edges smoother and the image works better with the edge detection modules. Gaussian blur can also be used for noise reduction. The Gaussian kernel G(x, y, $\sigma$ ) can be represented as depicted in Korn2000:
\cite{korn2000mathematical}
\begin{equation}
G(x,y,\sigma)=\frac{1}{2\pi \sigma}exp(-\frac{x^2+y^2}{2\sigma ^2}),
\end{equation}
where the two free co-ordinates are represented by x, y and $\sigma$ is a parameter.
\\The Gaussian blur works by using a two dimensional Gaussian kernel that can be separated into two operations of the one dimensional kernel. This enables very fast implementation of multidimensional Gaussian filtering \cite{korn2000mathematical}.  

\item[Gaussian Sharpen]\hfill \\
A Gaussian sharpen image module was implemented. This method was based on the Gaussian blur method as described above. It works by implementing a Gaussian blur on the source image, and then subtracting the Gaussian blurred image from the original image. This results in a sharpening of the image. 

\item[Image Fusion]\hfill \\
Several situations in image processing require both high spatial and high spectral information in a single image. However, this is not always possible due to design or observational limitations.
To overcome these limitations, image fusion techniques are applied.

Naiad's vision system implements single-source image fusion techniques to enhance image clarity and quality. However, this module is not used by default due to cost in terms of execution time. But with time, as the FPGA becomes usable, this module could be easily enabled.

For future, it is being planned to have a multi-focus fusion module in place to work alongside the stereo-vision system to help find objects more efficiently \cite{article:Zheng2010} \cite{article:Zhang2010} \cite{article:Shutao}.

\item[Enhance Colors]\hfill \\
This function was implemented by converting an image to a Hue Saturation Intensity (HSI) image, and then scrolling through each pixel in the image and adding a number to the intensity to increase the intensity of each pixel. This enhanced the colors of the image. This function was particularly useful when detecting red templates. When pictures of the red templates were taken it was observed that in parts of the template the red appeared a little faded. The enhance colors module worked a treat in enhancing the red color which led to better results when the canny and contour modules were run on the template. 
\end{description}

\subsubsection{Interpreting Image} 
After the image preprocessing modules of the vision system have completed, the image processing modules work on feature and object detection. The following modules were implemented to search for features on a given image. 
\begin{description}
\item[Threshold]\hfill \\
The vision system has the ability to threshold the input image to filter for any specified range of the visual spectrum. 
It works on HSI images, and it was particularly challenging to filter the color red as the hue values see a wrap-around effect for red, meaning that the spectrum begins at red and ends at red.

This problem was solved by working around the easily detected colors. To detect red, firstly yellow and green were filtered separately. Then the orange component was filtered from a bitwise OR-combination of the previous outputs. The resultant output is the red component.
the OpenCV function inRange is used here.

\item[Canny Edge Detection]\hfill \\
The Canny edge detection algorithm \cite{Canny:1986:CAE:11274.11275}  was used for detecting edges in the images. Canny states that the optimal detector for step edges would be convolution with a symmetric Gaussian and directional second spatial derivatives \cite{books/sp/Cipolla96}. The Canny algorithm is based on three main objectives \cite{Nixon:2008:FEI:1571711}:
\begin{enumerate}
  \item The response to noise is reduced using Gaussian filtering
  \item Use non-maximum suppression to obtain good accuracy (only use points from the top of a ridge of edge data, whilst suppressing all others)
  \item Eliminate multiple responses to a single edge by providing a single response
\end{enumerate}

The Canny module implemented takes a grey scale input and outputs a binary image with the estimated lines are white on a black background.

\item[Contour Tracing]\hfill \\
The contour module used was based on the OpenCV contour module. This module is based on Suzuki and Abe which analyzes the topological structure of binary
images by following the detected borders \cite{article:suzuki}. 
The contour module searches through the canny input and finds contours on the image. This can then be used in shape or template recognition modules.

\item[Approx Poly]\hfill \\
After the contours were found the OpenCV function approx poly was used to approximate the curves found in the contour function. The approxpoly function is based on the Douglas-Peucker algorithm \cite{article:douglas}. This was initially used to find shapes in test images supplied to the vision system. If three contours are found the shape is assumed to be a triangle, if four contours are found the shape is assumed to be a square. This was later replaced by OpenCV's shape matching function which provided more accurate results.

\item[Good Features to Track]\hfill \\
OpenCV's good features to track module was implemented to use with optical flow. This yielded better results with optical flow than from using optical flow with contours. This module is based on the Shi-Tomasi algorithm \cite{article:tomasi}. This module uses the algorithm to find the most prominent corners in an image region. The method also shows comparatively better results than the Harris corner algorithm \cite{article:tomasi}.

\item[Optical Flow Tracking]\hfill \\
Optical flow tracking was implemented to use as part of velocity estimation in the vision system. Optical flow tracking in OpenCV uses the Lucas-Kanade method and uses a sparse iterative method \cite{web:opticalFlow}, which computes optical flow for a sparse feature set using features detected in the good features to track module described above. This module was used in to find an estimated velocity for the AUV.

\end{description}

\subsubsection{Comprehending Features Found} 
When features on the image have been found, the vision system uses the features collected to interpret the image. The following modules were implemented to look for certain shapes and templates.
\begin{description}
\item[Hough Circles]\hfill \\
OpenCV's hough circles function was implemented to enable the ability to detect circles. The Hough circles module is based on the Hough circle transform as described in Yuen \cite{article:yuen}. 

\item[Hough Lines]\hfill \\
The AUV was equipped with line detection capability by implementing OpenCV's Hough lines module. The module is based on Matas2000 \cite{article:matas}. This can be used to find objects with parallel lines like pipes etc.

\item[Template Matching]\hfill \\
OpenCV's matchShapes function was used for the object recognition part of the vision system. This function is based on Hu Invariants \cite{article:hu}. The matchShapes module is also scale invariant.

\item[Velocity Estimation]\hfill \\
The velocity estimation module implemented was based on OpenCV's optical flow module. It has two modes. First it checks to see if any ``large" objects are detected (pipes, barrels etc.) and if they have been detected it uses optical flow to find the average difference of position of the features, and divides by the image capture rate to give an estimated velocity in pixels per second.
 
The second mode is used if there are no large objects detected. It then uses optical flow to search for particles in the water, or on the floor of the pool/seabed, and estimates the velocity like in mode 1 except using the small particles positional difference. The reason for the modes is that smaller particles are harder to pick up, and sometimes light in the water can be mistaken for particles and have a negative affect the velocity estimation. Therefore it is more reliable to estimate optical flow using features of a larger object, and the first mode is preferred.
\end{description}

\subsubsection{Three Dimensional Reconstruction} 
Point Cloud Library, PCL, was chosen for the three dimensional reconstruction part of the vision system. Due to timing constraints only a few basic filters and PCL's greedy surface reconstruction algorithm were implemented through Ada.
\begin{description}
\item[Converting to Ada.]\hfill \\
PCL was converted to Ada in much the same way as OpenCV. Modules were written in C++ and then bound to Ada \cite{web:newAdaBindings}. The PCL modules could then be called from Ada.

\item[Down-sampling a Point Cloud]\hfill \\
The point clouds to be reconstructed to 3-D must undergo stages before the actual reconstruction can occur. Different filters were implemented to clean the point cloud to obtain better results.
\\The first PCL module implemented was a down-sampling module \cite{web:pclVoxel} . The down-sampling module is based on a voxelized grid approach. The size of the voxel is specified and all points in that voxel are approximated with their centroid.	

\item[Remove Outliers]\hfill \\
A statistical outlier removal module was constructed to remove outliers from point clouds \cite{web:pclOutliers}. The module works by computing the mean distances of every point to their neighbours. Then all points whose mean distances are outside a defined interval are assumed to be outliers and are removed from the point cloud.

\item[Conditional Euclidean Segmentation]\hfill \\
This module was based on the conditional euclidean segmentation tutorial on the Point Cloud Library website\cite{web:PCL}. The tutorial combines euclidean cluster extraction \cite{RusuDoctoralDissertation} and region growing segmentation \cite{web:pclRegionGrowing}. Euclidean segmentation breaks large point clouds down into smaller, more manageable data units. This improves computational time for the module. The region growing segmentation algorithm then searches for points on the same smooth surface. The segmented point cloud is then plotted using different colors to show the detected object clusters.

\item[Triangulation Mesh]\hfill \\
The PCL greedy triangulation mesh algorithm was implemented. This constructs a triangular mesh around the point cloud, effectively turning the point cloud from a group of points to a reconstructed surface. It works by finding a list of points a mesh can be constructed from and expands the list to include suitable points \cite{Marton09ICRA}. 
 
\end{description}